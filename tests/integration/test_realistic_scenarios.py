# tests/integration/test_realistic_scenarios.py

import pytest
import app
from unittest.mock import patch

# Importe o 'workflow' compilado do seu fluxo principal e o criador de estado
# O caminho pode precisar de ajuste dependendo da sua estrutura de pastas
from app.core.langgraph_flow import workflow as graph
from app.core.state.models import create_initial_cecilia_state


@pytest.mark.asyncio
async def test_handles_complex_initial_message_end_to_end(patch_redis, mock_delivery, mock_gemini, mock_openai):
    """
    üß™ TESTE DE INTEGRA√á√ÉO DE PONTA A PONTA (Cen√°rio Realista)

    Este teste valida o fluxo completo para a mensagem complexa:
    "ol√°, meu nome √© Gabriel, gostaria de informa√ß√µes sobre o kumon de matem√°tica"

    Ele garante que:
    1. O roteador e o classificador trabalhem juntos para extrair todas as entidades.
    2. O n√≥ correto (`information_node`) seja ativado.
    3. O estado seja corretamente preenchido com as entidades extra√≠das.
    4. A resposta final seja uma "resposta combinada" inteligente, respondendo √†
       pergunta e continuando a qualifica√ß√£o.
    """
    print("\n--- üß™ INICIANDO TESTE DE CEN√ÅRIO REALISTA ---")

    # --- ARRANGE (Prepara√ß√£o) ---

    # 1. A mensagem complexa do usu√°rio
    user_message = "ol√°, meu nome √© Gabriel, gostaria de informa√ß√µes sobre o kumon de matem√°tica"

    # 2. O estado inicial da conversa, como se viesse de um novo webhook
    initial_state = create_initial_cecilia_state(
        phone_number="5511999999999",
        user_message=user_message,
        instance="test_instance",
    )
    # Adicionamos campos necess√°rios para compatibilidade com o grafo
    initial_state['text'] = user_message
    initial_state['phone'] = "5511999999999"
    initial_state['message_id'] = "test_msg_123"

    # --- ACT (Execu√ß√£o) ---

    # 3. Invocamos o grafo completo, da mesma forma que a aplica√ß√£o faria
    # Usamos um patch para garantir que o LLM n√£o seja chamado de verdade,
    # tornando o teste mais r√°pido e previs√≠vel. Simulamos uma resposta ideal.
    with patch('app.core.llm.openai_adapter.OpenAIClient.chat') as mock_llm_chat:
        # Simulamos a resposta da IA para a "resposta combinada" final
        mock_llm_chat.return_value = (
            "Ol√° Gabriel! O Kumon de Matem√°tica √© um m√©todo individualizado que fortalece o racioc√≠nio. "
            "Para que eu possa te ajudar melhor, o Kumon √© para voc√™ mesmo ou para outra pessoa?"
        )

        final_state = await graph.ainvoke(initial_state)

    # --- ASSERT (Verifica√ß√£o) ---

    print(f"\n‚úÖ DADOS EXTRA√çDOS NO ESTADO FINAL: {final_state.get('collected_data')}")
    print(f"‚úÖ RESPOSTA FINAL DO BOT: {final_state.get('last_bot_response')}")

    # 4. Verificamos se as entidades foram extra√≠das corretamente
    # Esta √© a prova de que o GeminiClassifier funcionou.
    collected_data = final_state.get("collected_data", {})
    assert collected_data.get("parent_name") == "Gabriel", \
        "Deveria ter extra√≠do 'Gabriel' como parent_name"
    
    assert "Matem√°tica" in collected_data.get("program_interests", []), \
        "Deveria ter extra√≠do 'Matem√°tica' como program_interests"

    # 5. Verificamos se a resposta final √© a "resposta combinada" inteligente
    # Esta √© a prova de que o information_node (ou o n√≥ que foi ativado) funcionou.
    final_response = final_state.get("last_bot_response", "").lower()
    
    # Garante que a parte informativa da resposta est√° presente
    assert "m√©todo individualizado" in final_response, \
        "A resposta deveria conter a informa√ß√£o solicitada sobre o Kumon."

    # Garante que a parte de continua√ß√£o da qualifica√ß√£o est√° presente
    assert "para voc√™ mesmo ou para outra pessoa" in final_response, \
        "A resposta deveria continuar a qualifica√ß√£o perguntando sobre o benefici√°rio."

    print("\n--- üéØ TESTE DE CEN√ÅRIO REALISTA CONCLU√çDO COM SUCESSO ---")